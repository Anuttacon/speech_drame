# Zero-shot Speech Drame Evaluation

This module provides zero-shot evaluation capabilities for AI models on speech drama generation tasks. It supports multiple state-of-the-art audio language models and comprehensive evaluation metrics.

## üéØ Overview

Zero-shot evaluation tests models without any task-specific training, providing a baseline assessment of their natural capabilities for speech drama generation and evaluation.

## üèóÔ∏è Architecture

```
zeroshot/
‚îú‚îÄ‚îÄ models/           # Audio LLM implementations
‚îÇ   ‚îú‚îÄ‚îÄ base_audiollm.py      # Abstract base class
‚îÇ   ‚îú‚îÄ‚îÄ gemini25_pro.py       # Google Gemini 2.5 Pro
‚îÇ   ‚îú‚îÄ‚îÄ gpt4o.py             # OpenAI GPT-4o
‚îÇ   ‚îú‚îÄ‚îÄ qwen25_omni.py       # Alibaba Qwen2.5 Omni
‚îÇ   ‚îú‚îÄ‚îÄ qwen2_audio.py       # Alibaba Qwen2 Audio
‚îÇ   ‚îî‚îÄ‚îÄ kimi_audio.py        # Moonshot Kimi Audio
‚îú‚îÄ‚îÄ prompts/          # Evaluation prompts and schemas
‚îÇ   ‚îú‚îÄ‚îÄ all_prompt.py        # General evaluation prompts
‚îÇ   ‚îú‚îÄ‚îÄ all_archetype_prompt.py  # Character archetype prompts
‚îÇ   ‚îú‚îÄ‚îÄ sep_prompt.py        # Separate dimension prompts
‚îÇ   ‚îú‚îÄ‚îÄ sep_archetype_prompt.py  # Separate archetype prompts
‚îÇ   ‚îî‚îÄ‚îÄ schema.py            # JSON schemas for structured output
‚îú‚îÄ‚îÄ pyscripts/        # Python evaluation scripts
‚îÇ   ‚îú‚îÄ‚îÄ eval.py              # Main evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ score.py             # Scoring utilities
‚îÇ   ‚îú‚îÄ‚îÄ score_avg.py         # Average score calculation
‚îÇ   ‚îú‚îÄ‚îÄ arche_score.py       # Archetype-specific scoring
‚îÇ   ‚îú‚îÄ‚îÄ arche_score_avg.py   # Archetype average scoring
‚îÇ   ‚îú‚îÄ‚îÄ utils.py             # Utility functions
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint_utils.py  # Checkpoint management
‚îî‚îÄ‚îÄ scripts/          # Shell scripts for batch evaluation
    ‚îú‚îÄ‚îÄ eval_*.sh            # Model-specific evaluation scripts
    ‚îú‚îÄ‚îÄ score_analysis*.sh   # Score analysis scripts
    ‚îî‚îÄ‚îÄ eval_with_checkpoints.sh  # Checkpoint-based evaluation
```

## üöÄ Quick Start

### Prerequisites

1. **Environment Setup**
   ```bash
   conda activate zeroshot
   export PYTHONPATH=/path/to/speech_drame/zeroshot:${PYTHONPATH}
   ```

2. **API Keys** (for cloud-based models)
   ```bash
   export OPENAI_API_KEY="your-openai-key"
   export GOOGLE_API_KEY="your-google-key"
   # Add other API keys as needed
   ```

### Basic Usage

#### Single Model Evaluation
```bash
python pyscripts/eval.py \
    --model_name gemini25 \
    --model_tag gemini-2.5-pro \
    --data_path ../data/realism_data/test.jsonl \
    --output_path results.jsonl \
    --prompt_type combined \
    --prompt_version v2 \
    --batch_size 4 \
    --num_workers 4
```

#### Batch Evaluation with Scripts
```bash
# Evaluate Gemini 2.5 Pro
bash scripts/eval_gemini.sh

# Evaluate GPT-4o
bash scripts/eval_gpt.sh

# Evaluate Qwen2.5 Omni
bash scripts/eval_qwen25.sh
```

## üìä Evaluation Types

The zero-shot framework supports two distinct evaluation approaches:

### üé≠ **Realism Evaluation**
Evaluates speech drama generation for realistic character portrayal and narrative consistency.

#### 1. Combined Realism Evaluation
Evaluates all 11 dimensions in a single prompt:
```bash
--prompt_type combined
--data_path ../data/realism_data/test.jsonl
```

#### 2. Separate Realism Evaluation
Evaluates each dimension individually:
```bash
--prompt_type separate
--data_path ../data/realism_data/test.jsonl
```

### üé™ **Archetype Evaluation**
Evaluates speech generation for character archetype consistency and role-playing accuracy.

#### 1. Combined Archetype Evaluation
Evaluates all 4 dimensions in a single prompt:
```bash
--prompt_type archetype_combined
--data_path ../data/archetype_data/test.jsonl
```

#### 2. Separate Archetype Evaluation
Evaluates each dimension individually:
```bash
--prompt_type archetype_separate
--data_path ../data/archetype_data/test.jsonl
```

## ü§ñ Supported Models

| Model | Implementation | API Type | Schema Support |
|-------|---------------|----------|----------------|
| **Gemini 2.5 Pro** | `gemini25_pro.py` | Google Vertex AI | ‚úÖ |
| **GPT-4o** | `gpt4o.py` | OpenAI API | ‚úÖ |
| **Qwen2.5 Omni** | `qwen25_omni.py` | Local/API | ‚úÖ |
| **Qwen2 Audio** | `qwen2_audio.py` | Local/API | ‚úÖ |
| **Kimi Audio** | `kimi_audio.py` | Moonshot API | ‚úÖ |

## üìà Evaluation Metrics

### üé≠ **Realism Evaluation Metrics (11 Dimensions)**
1. **Pitch Dynamics** - Pitch contour variety and appropriateness
2. **Rhythmic Naturalness** - Flow, timing, pause quality
3. **Stress Emphasis** - Syllable/word emphasis clarity
4. **Emotion Accuracy** - Emotion-label matching
5. **Emotion Intensity** - Emotion strength
6. **Emotional Dynamic Range** - Emotion variation
7. **Voice Identity Matching** - Vocal identity consistency
8. **Trait Embodiment** - Character trait presence
9. **Local Scene Fit** - Immediate scene appropriateness
10. **Global Story Fit** - Overall story consistency
11. **Semantic Matchness** - Content-scene goal alignment

### üé™ **Archetype Evaluation Metrics (4 Dimensions)**
1. **Content Pass** - Binary pass/fail for basic requirements (length, relevance, language)
2. **Audio Quality** - Technical quality assessment (artifacts, glitches)
3. **Human Likeness** - Context-free naturalness of speech delivery
4. **Appropriateness** - Context-dependent fit to role and scene requirements

**Scoring Scale:**
- **Realism**: All dimensions scored 1-5 (1 = poor, 5 = excellent)
- **Archetype**: Content Pass (true/false), others scored 1-5

## üîß Advanced Features

### Checkpointing
Resume interrupted evaluations:
```bash
python pyscripts/eval.py \
    --checkpoint_dir checkpoints/ \
    --checkpoint_interval 10 \
    --resume_from checkpoints/checkpoint_file.json
```

### Schema Validation
Use structured output for better reliability:
```bash
--use_schema true
```

### Content Pass Mode
For models supporting binary classification:
```bash
--prompt_type archetype_separate  # Automatically detects content_pass
```

## üìä Results Analysis

### Generate Average Scores
```bash
python pyscripts/score_avg.py \
    --input_dir results/ \
    --output_file analysis.json
```

### Archetype-Specific Analysis
```bash
python pyscripts/arche_score.py \
    --input_dir results/ \
    --output_file archetype_analysis.json
```

### Batch Analysis with Scripts
```bash
# General score analysis
bash scripts/score_analysis.sh

# Archetype score analysis
bash scripts/score_analysis_arche.sh

# Individual score analysis
bash scripts/score_individual.sh
```

## üìÅ Data Formats

### üé≠ **Realism Evaluation Data**
**Input Format:**
```json
{
  "id": "role-eval_v1_0000",
  "local_scene": "Character description and scene context",
  "char_age": "Adult",
  "char_style": "Playful, Charismatic, Confident",
  "char_profile": "Detailed character background",
  "transcript": "Spoken text content",
  "wav_path": "path/to/audio/file.wav",
  "annotations": {
    "pitch_variation": [5, 3, 4, 5, 5, 5],
    "rhythmic_naturalness": [5, 4, 4, 5, 2, 5],
    "emotion_accuracy": [4, 4, 4, 5, 5, 4],
    "trait_embodiment": [5, 5, 5, 4, 5, 5]
  }
}
```

**Output Format:**
```json
{
  "id": "role-eval_v1_0000",
  "model_name": "gemini25",
  "response": "{\"pitch_dynamics\": 4.0, \"rhythmic_naturalness\": 3.0, \"emotion_accuracy\": 4.0, ...}",
  "audio_path": "path/to/audio/file.wav"
}
```

### üé™ **Archetype Evaluation Data**
**Input Format:**
```json
{
  "id": "250814_zh_SocialIdentity_0051",
  "question": "‰Ω†ÊòØÊãñÂª∂ÁóáÊÇ£ËÄÖ„ÄÇÊúãÂèãËØ¢ÈóÆ‰Ω†‰πãÂâçÁ≠îÂ∫îÂ∏ÆÂøôÂÅöÁöÑ‰∏Ä‰ª∂‰∫ãËøõÂ±ïÂ¶Ç‰Ωï„ÄÇ‰Ω†ËØ¥Ôºö",
  "wav_path": "path/to/audio/file.wav",
  "annotations": {
    "content_pass": [true, true, true, true, true, true],
    "audio_quality": [4, 5, 4, 4, 4, 4],
    "human_likeness": [3, 4, 3, 4, 4, 4],
    "appropriateness": [4, 4, 4, 4, 5, 5]
  }
}
```

**Output Format:**
```json
{
  "id": "250814_zh_SocialIdentity_0051",
  "model_name": "gemini25",
  "response": "{\"content_pass\": true, \"audio_quality\": 4, \"human_likeness\": 3, \"appropriateness\": 4}",
  "audio_path": "path/to/audio/file.wav"
}
```

## üõ†Ô∏è Configuration

### Model Configuration
Each model implementation supports:
- Custom system prompts
- Retry logic with exponential backoff
- Batch processing with thread pools
- Schema-based structured output

### Evaluation Configuration
- **Batch Size**: Control memory usage and API rate limits
- **Num Workers**: Parallel processing threads
- **Prompt Version**: v1 (basic) or v2 (detailed)
- **Checkpoint Interval**: Save progress frequency

## üîç Troubleshooting

### Common Issues

1. **API Rate Limits**
   - Reduce `batch_size` and `num_workers`
   - Implement longer delays between requests

2. **Memory Issues**
   - Process smaller batches
   - Use checkpointing for large datasets

3. **Schema Validation Errors**
   - Check model compatibility with `--use_schema`
   - Fall back to regular generation if needed

### Debug Mode
Enable verbose logging:
```bash
export PYTHONPATH=/path/to/speech_drame/zeroshot:${PYTHONPATH}
python -u pyscripts/eval.py [args] 2>&1 | tee evaluation.log
```

## üìö API Reference

### BaseAudioLLM
Abstract base class for all audio LLM implementations:
```python
class BaseAudioLLM(ABC):
    @abstractmethod
    def generate(self, prompt: str, audio_path: str) -> Dict[str, Any]
    
    @abstractmethod
    def batch_generate(self, prompts: List[str], audio_paths: List[str]) -> List[Dict[str, Any]]
```

### Model-Specific Methods
- `evaluate_with_schema()`: Schema-based evaluation
- `_ask_with_retry()`: Retry logic with backoff
- Custom prompt handling and response parsing

## ü§ù Contributing

1. **Adding New Models**
   - Inherit from `BaseAudioLLM`
   - Implement required methods
   - Add to model selection logic in `eval.py`

2. **Adding New Prompts**
   - Create prompt functions in appropriate module
   - Add to prompt selection logic
   - Update schema definitions if needed

3. **Improving Evaluation**
   - Enhance scoring algorithms
   - Add new evaluation dimensions
   - Improve error handling and robustness

## üìÑ License

This module is part of the Speech Drama Evaluation Framework and follows the same licensing terms.

---

For more detailed information, see the main project README and individual model documentation.
