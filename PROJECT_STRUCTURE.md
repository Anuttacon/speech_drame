# Project Structure Documentation

This document provides a comprehensive overview of the Speech Drama Evaluation Framework project structure.

## ğŸ“ Root Directory Structure

```
speech_drame/
â”œâ”€â”€ README.md                    # Main project documentation
â”œâ”€â”€ PROJECT_STRUCTURE.md         # This file - project structure documentation
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ data/                        # Evaluation datasets
â”œâ”€â”€ zeroshot/                    # Zero-shot evaluation framework
â”œâ”€â”€ fewshot/                     # Few-shot evaluation framework
â”œâ”€â”€ finetuning/                  # Fine-tuning framework
â””â”€â”€ scripts/                     # Utility scripts and tools
```

## ğŸ“Š Data Directory

```
data/
â”œâ”€â”€ archetype_data/              # Character archetype evaluation data
â”‚   â”œâ”€â”€ test.jsonl              # Test dataset for archetype evaluation
â”‚   â””â”€â”€ *.wav                   # Audio files for archetype evaluation
â””â”€â”€ realism_data/               # Realism evaluation data
    â”œâ”€â”€ test.jsonl              # Test dataset for realism evaluation
    â””â”€â”€ *.wav                   # Audio files for realism evaluation
```

**Data Format:**
- **JSONL files**: Each line contains a JSON object with evaluation data
- **Audio files**: WAV format audio files for speech evaluation
- **Annotations**: Human-annotated scores for various evaluation dimensions

## ğŸ¯ Zero-shot Evaluation Framework

```
zeroshot/
â”œâ”€â”€ README.md                    # Zero-shot evaluation documentation
â”œâ”€â”€ models/                      # Audio LLM implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_audiollm.py        # Abstract base class for audio LLMs
â”‚   â”œâ”€â”€ gemini25_pro.py         # Google Gemini 2.5 Pro implementation
â”‚   â”œâ”€â”€ gpt4o.py               # OpenAI GPT-4o implementation
â”‚   â”œâ”€â”€ qwen25_omni.py         # Alibaba Qwen2.5 Omni implementation
â”‚   â”œâ”€â”€ qwen2_audio.py         # Alibaba Qwen2 Audio implementation
â”‚   â”œâ”€â”€ kimi_audio.py          # Moonshot Kimi Audio implementation
â”‚   â””â”€â”€ kimiaudio_utility/      # Kimi Audio utility modules
â”œâ”€â”€ prompts/                     # Evaluation prompts and schemas
â”‚   â”œâ”€â”€ all_prompt.py           # General evaluation prompts
â”‚   â”œâ”€â”€ all_archetype_prompt.py # Character archetype prompts
â”‚   â”œâ”€â”€ sep_prompt.py           # Separate dimension prompts
â”‚   â”œâ”€â”€ sep_archetype_prompt.py # Separate archetype prompts
â”‚   â””â”€â”€ schema.py               # JSON schemas for structured output
â”œâ”€â”€ pyscripts/                   # Python evaluation scripts
â”‚   â”œâ”€â”€ eval.py                 # Main evaluation script
â”‚   â”œâ”€â”€ score.py                # Scoring utilities
â”‚   â”œâ”€â”€ score_avg.py            # Average score calculation
â”‚   â”œâ”€â”€ arche_score.py          # Archetype-specific scoring
â”‚   â”œâ”€â”€ arche_score_avg.py      # Archetype average scoring
â”‚   â”œâ”€â”€ utils.py                # Utility functions
â”‚   â””â”€â”€ checkpoint_utils.py     # Checkpoint management utilities
â””â”€â”€ scripts/                     # Shell scripts for batch evaluation
    â”œâ”€â”€ eval_*.sh               # Model-specific evaluation scripts
    â”œâ”€â”€ score_analysis*.sh      # Score analysis scripts
    â””â”€â”€ eval_with_checkpoints.sh # Checkpoint-based evaluation
```

## ğŸ¯ Few-shot Evaluation Framework

```
fewshot/
â”œâ”€â”€ README.md                    # Few-shot evaluation documentation
â”œâ”€â”€ models/                      # Audio LLM implementations (shared with zeroshot)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_audiollm.py        # Abstract base class for audio LLMs
â”‚   â”œâ”€â”€ gemini25_pro.py         # Google Gemini 2.5 Pro implementation
â”‚   â”œâ”€â”€ gpt4o.py               # OpenAI GPT-4o implementation
â”‚   â”œâ”€â”€ qwen25_omni.py         # Alibaba Qwen2.5 Omni implementation
â”‚   â”œâ”€â”€ qwen2_audio.py         # Alibaba Qwen2 Audio implementation
â”‚   â”œâ”€â”€ kimi_audio.py          # Moonshot Kimi Audio implementation
â”‚   â””â”€â”€ kimiaudio_utility/      # Kimi Audio utility modules
â”œâ”€â”€ prompts/                     # Few-shot evaluation prompts
â”‚   â”œâ”€â”€ all_prompt.py           # General few-shot prompts
â”‚   â”œâ”€â”€ all_archetype_prompt.py # Character archetype few-shot prompts
â”‚   â”œâ”€â”€ sep_prompt.py           # Separate dimension few-shot prompts
â”‚   â”œâ”€â”€ sep_archetype_prompt.py # Separate archetype few-shot prompts
â”‚   â””â”€â”€ schema.py               # JSON schemas for structured output
â”œâ”€â”€ pyscripts/                   # Python evaluation scripts
â”‚   â”œâ”€â”€ eval.py                 # Main few-shot evaluation script
â”‚   â”œâ”€â”€ score.py                # Scoring utilities
â”‚   â”œâ”€â”€ score_avg.py            # Average score calculation
â”‚   â”œâ”€â”€ arche_score.py          # Archetype-specific scoring
â”‚   â”œâ”€â”€ arche_score_avg.py      # Archetype average scoring
â”‚   â””â”€â”€ utils.py                # Utility functions
â””â”€â”€ scripts/                     # Shell scripts for batch evaluation
    â”œâ”€â”€ eval_*.sh               # Model-specific evaluation scripts
    â””â”€â”€ score_analysis*.sh      # Score analysis scripts
```

## ğŸ¯ Fine-tuning Framework

```
finetuning/
â”œâ”€â”€ README.md                    # Fine-tuning documentation
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ train_sft.py            # Main SFT training script
â”‚   â”œâ”€â”€ train_grpo.py           # GRPO training script
â”‚   â”œâ”€â”€ test_*.py               # Testing scripts for different tasks
â”‚   â”œâ”€â”€ analyze_role_results.py # Result analysis script
â”‚   â”œâ”€â”€ dataset/                 # Dataset handling
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py          # Audio dataset implementation
â”‚   â”‚   â”œâ”€â”€ prompt.py           # Prompt generation utilities
â”‚   â”‚   â”œâ”€â”€ archetype_prompt.py # Archetype-specific prompts
â”‚   â”‚   â””â”€â”€ prompt_variations.json # Prompt variations
â”‚   â”œâ”€â”€ trainer/                 # Custom trainers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sft_trainer.py      # SFT trainer implementation
â”‚   â”‚   â”œâ”€â”€ grpo_trainer.py     # GRPO trainer implementation
â”‚   â”‚   â””â”€â”€ grpo_trainer_new.py # Updated GRPO trainer
â”‚   â””â”€â”€ utils/                   # Utility modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ peft_utils.py       # PEFT configuration utilities
â”‚       â”œâ”€â”€ rewards.py          # Reward computation
â”‚       â””â”€â”€ show_acc.py         # Accuracy visualization
â”œâ”€â”€ conf/                        # Configuration files
â”‚   â”œâ”€â”€ sft_ft.yaml             # SFT fine-tuning configuration
â”‚   â”œâ”€â”€ sft_lora.yaml           # SFT with LoRA configuration
â”‚   â”œâ”€â”€ ds_zero1.json           # DeepSpeed ZeRO-1 configuration
â”‚   â”œâ”€â”€ ds_zero2.json           # DeepSpeed ZeRO-2 configuration
â”‚   â””â”€â”€ ds_zero3.json           # DeepSpeed ZeRO-3 configuration
â”œâ”€â”€ scripts/                     # Training and evaluation scripts
â”‚   â”œâ”€â”€ parse_result_*.py       # Result parsing utilities
â”‚   â”œâ”€â”€ prompt_generation.py    # Prompt generation scripts
â”‚   â””â”€â”€ show_result_folder.py   # Result visualization
â””â”€â”€ run_*.sh                     # Shell scripts for training and testing
    â”œâ”€â”€ run_archetype_train.sh  # Archetype training
    â”œâ”€â”€ run_archetype_test.sh   # Archetype testing
    â”œâ”€â”€ run_realism_train.sh    # Realism training
    â””â”€â”€ run_realism_test.sh     # Realism testing
```

## ğŸ› ï¸ Scripts Directory

```
scripts/
â”œâ”€â”€ README.md                    # Scripts documentation
â””â”€â”€ format_code.py              # Code formatting utility
```

## ğŸ“‹ Key Components

### 1. Model Implementations
- **BaseAudioLLM**: Abstract base class defining the interface for audio LLMs
- **Model-specific implementations**: Concrete implementations for different AI models
- **Utility modules**: Supporting code for specific model integrations

### 2. Prompt Engineering
- **General prompts**: Standard evaluation prompts for all dimensions
- **Archetype prompts**: Character-specific evaluation prompts
- **Separate prompts**: Individual dimension evaluation prompts
- **Schema definitions**: JSON schemas for structured model outputs

### 3. Evaluation Scripts
- **Main evaluation**: Core evaluation logic for different approaches
- **Scoring utilities**: Functions for processing and analyzing results
- **Batch processing**: Shell scripts for running evaluations at scale
- **Checkpointing**: Utilities for resuming interrupted evaluations

### 4. Training Framework
- **SFT training**: Supervised fine-tuning implementation
- **GRPO training**: Group Relative Policy Optimization
- **Dataset handling**: Audio-text dataset processing
- **PEFT support**: Parameter-efficient fine-tuning methods
- **Configuration**: YAML and JSON configuration files

### 5. Data Processing
- **Audio handling**: Audio file loading and preprocessing
- **Annotation processing**: Human annotation parsing and validation
- **Format conversion**: Data format standardization
- **Quality control**: Data validation and cleaning

## ğŸ”„ Data Flow

### Evaluation Flow
1. **Data Loading**: Load evaluation datasets and audio files
2. **Prompt Generation**: Create evaluation prompts based on task type
3. **Model Inference**: Run models on audio-text pairs
4. **Result Processing**: Parse and validate model outputs
5. **Scoring**: Calculate evaluation metrics and scores
6. **Analysis**: Generate performance reports and comparisons

### Training Flow
1. **Data Preparation**: Process training datasets
2. **Model Loading**: Load base models and configurations
3. **Training Loop**: Execute training with monitoring
4. **Checkpointing**: Save model states and metrics
5. **Evaluation**: Test trained models on validation data
6. **Deployment**: Package models for inference

## ğŸ¯ Evaluation Dimensions

The framework supports two distinct evaluation approaches with different dimensional rubrics:

### ğŸ­ **Realism Evaluation (11 Dimensions)**
Evaluates speech drama generation for realistic character portrayal and narrative consistency:

1. **Pitch Dynamics**: Pitch contour variety and appropriateness
2. **Rhythmic Naturalness**: Flow, timing, pause quality
3. **Stress Emphasis**: Syllable/word emphasis clarity
4. **Emotion Accuracy**: Emotion-label matching
5. **Emotion Intensity**: Emotion strength
6. **Emotional Dynamic Range**: Emotion variation
7. **Voice Identity Matching**: Vocal identity consistency
8. **Trait Embodiment**: Character trait presence
9. **Local Scene Fit**: Immediate scene appropriateness
10. **Global Story Fit**: Overall story consistency
11. **Semantic Matchness**: Content-scene goal alignment

### ğŸª **Archetype Evaluation (4 Dimensions)**
Evaluates speech generation for character archetype consistency and role-playing accuracy:

1. **Content Pass**: Binary pass/fail for basic requirements (length, relevance, language)
2. **Audio Quality**: Technical quality assessment (artifacts, glitches)
3. **Human Likeness**: Context-free naturalness of speech delivery
4. **Appropriateness**: Context-dependent fit to role and scene requirements

## ğŸ”§ Configuration Management

### YAML Configuration Files
- **Training parameters**: Learning rates, batch sizes, epochs
- **Model settings**: Model paths, initialization parameters
- **PEFT configuration**: LoRA, QLoRA, and other PEFT methods
- **Logging settings**: TensorBoard, wandb, logging frequency

### JSON Configuration Files
- **DeepSpeed settings**: ZeRO optimization configurations
- **Dataset configurations**: Data loading and preprocessing settings
- **Evaluation settings**: Evaluation parameters and metrics

## ğŸ“Š Output Formats

### Evaluation Results
- **JSONL format**: One result per line for easy processing
- **Structured output**: Consistent format across all evaluation methods
- **Metadata**: Model information, timestamps, and configuration details

### Training Outputs
- **Model checkpoints**: Saved model states and weights
- **Training logs**: Loss curves, metrics, and performance data
- **Configuration backups**: Training configuration preservation

## ğŸš€ Deployment Considerations

### Model Serving
- **API endpoints**: RESTful interfaces for model inference
- **Batch processing**: Efficient handling of multiple requests
- **Resource management**: GPU memory and compute optimization

### Scalability
- **Distributed training**: Multi-GPU and multi-node support
- **Parallel evaluation**: Concurrent model evaluation
- **Caching**: Result caching for repeated evaluations

## ğŸ“ Documentation Standards

### Code Documentation
- **Docstrings**: Comprehensive function and class documentation
- **Type hints**: Python type annotations for better code clarity
- **Comments**: Inline comments for complex logic

### User Documentation
- **README files**: Module-specific documentation
- **API reference**: Detailed function and class documentation
- **Examples**: Usage examples and tutorials

## ğŸ” Quality Assurance

### Code Quality
- **Formatting**: Consistent code style with black and isort
- **Linting**: Code quality checks with flake8
- **Type checking**: Static type analysis with mypy

### Testing
- **Unit tests**: Individual component testing
- **Integration tests**: End-to-end workflow testing
- **Performance tests**: Benchmarking and profiling

---

This project structure supports a comprehensive evaluation framework for speech drama generation, providing multiple evaluation approaches and extensive configuration options for research and development.
